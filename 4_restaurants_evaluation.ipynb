{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating recommender systems"
      ],
      "metadata": {
        "id": "RDrVxX8XaJXl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRAvq-p9PqJv"
      },
      "source": [
        "We will look at the restaurant recommendations once more. To evaluate our recommender system, we will split our data into train and test sets again. This allows us to compare predictions with true values and evaluate how well our recommender performs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CGZsjXrNaHAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.&nbsp;Import data"
      ],
      "metadata": {
        "id": "azTi88N2adCh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDBPIi-gPqJo"
      },
      "outputs": [],
      "source": [
        "# Import the csv with the ratings.\n",
        "url = 'https://drive.google.com/file/d/1ptu4AlEXO4qQ8GytxKHoeuS1y4l_zWkC/view?usp=sharing' \n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "frame = pd.read_csv(path)\n",
        "\n",
        "users_items = pd.pivot_table(data=frame, \n",
        "                                 values='rating', \n",
        "                                 index='userID', \n",
        "                                 columns='placeID')\n",
        "\n",
        "users_items.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz73th7pPqJs"
      },
      "outputs": [],
      "source": [
        "users_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik67nizgPqJt"
      },
      "source": [
        "# 2.&nbsp;Train-test split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1&nbsp;Find all nonzero ratings"
      ],
      "metadata": {
        "id": "mWRW1wFNa0-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will help us make the train and test split.\n",
        "\n",
        "The `0.0` ratings cannot go to the test set. Therefore, we need to identify the non-zero ratings and make the split on them."
      ],
      "metadata": {
        "id": "dyctqmE8bBYd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqjHi9O6PqJw"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame that contains the positions of all nonzero ratings.\n",
        "ratings_pos = pd.DataFrame(\n",
        "    np.nonzero(np.array(users_items)),\n",
        "    ).T\n",
        "\n",
        "ratings_pos.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the columns.\n",
        "ratings_pos.columns = [\"row_pos\", \"column_pos\"]\n",
        "ratings_pos.head()"
      ],
      "metadata": {
        "id": "jtU7gQ9rcCNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRRLUhatPqJx"
      },
      "source": [
        "How shall we interpret the `rating_pos` DataFrame?\n",
        "\n",
        "The ratings at the positions [0, 31], [0, 32], [0, 75], [0, 81] etc. from the `users_items` DataFrame are not equal to zero.\n",
        "\n",
        "The value in the column `row_pos` corresponds to the row index in the `users_items` DataFrame, whereas the value in the column `column_pos` corresponds to the column index.\n",
        "\n",
        "Example [0,31]: This corresponds to a nonzero rating from the first user (userID = U1001). This is because this user's data is stored in the first row of the `users_items` DataFrame, with index `0`. The rating was for the restaurant in the column at position 31 (132825)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj_m4Cw1PqJy"
      },
      "outputs": [],
      "source": [
        "# Get the nonzero ratings from the positions above.\n",
        "users_items.iloc[0:1, [31, 32, 75, 81, 85]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjYKXms2PqJz"
      },
      "source": [
        "Let's find out how many non-zero values are in the `rating_pos` DataFrame. These are the candidates to take part in the train and test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKYZVEv7PqJz"
      },
      "outputs": [],
      "source": [
        "len(ratings_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45mZ6zqaPqJ0"
      },
      "source": [
        "## 2.2&nbsp;Make the train and test split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "m5o3u0zmqcRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiCDG2yLPqJ0"
      },
      "outputs": [],
      "source": [
        "# Split nonzero ratings into train and test sets.\n",
        "train_pos, test_pos = train_test_split(ratings_pos, \n",
        "                                       random_state=123, \n",
        "                                       test_size=.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joofAKddPqJ1"
      },
      "source": [
        "These values are in the train set..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zC7EEcpPqJ1"
      },
      "outputs": [],
      "source": [
        "train_pos.sort_values([\"row_pos\",\"column_pos\"]).head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kCb81SZPqJ1"
      },
      "source": [
        "...and these in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv_-SGt3PqJ1"
      },
      "outputs": [],
      "source": [
        "test_pos.sort_values([\"row_pos\",\"column_pos\"]).head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiTFvfzIPqJ1"
      },
      "source": [
        "Now we have two DataFrames called `train_pos` and `test_pos` which contain the rating positions in the `users_items` DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3&nbsp;Create the train DataFrame"
      ],
      "metadata": {
        "id": "XoWWce68tu9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train and test datasets will both have the same shape as the `users_items`DataFrame. Most of their values will be zero, except for the values in the positions stored inside the `train_pos` set for the train dataset, and the `test_pos`set for the test dataset."
      ],
      "metadata": {
        "id": "l1FCyhYPrsAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the users-items DataFrame and set all values to zero.\n",
        "train = users_items.copy()\n",
        "\n",
        "for column in train.columns:\n",
        "  train[column].values[:] = 0"
      ],
      "metadata": {
        "id": "rbrbO_D-etPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum the values in the DataFrame to check that all of them are equal to zero.\n",
        "train.sum().sum()"
      ],
      "metadata": {
        "id": "Ktm0CDd9Lh09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the nonzero positions in the train dataset.\n",
        "# Get the corresponding rating for each nonzero position from the users_items DataFrame.\n",
        "# Insert that value into the newly created DataFrame at the same position.\n",
        "for pos in train_pos.values: \n",
        "    index = pos[0]\n",
        "    col = pos[1]\n",
        "    train.iloc[index, col] = users_items.iloc[index, col]"
      ],
      "metadata": {
        "id": "zBqMKRnbjuRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_tjTbU2PqJ2"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many ratings from our user `U1001` fell into the train set?"
      ],
      "metadata": {
        "id": "foYI-bIKZefF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.iloc[0:1, [31, 32, 75, 81, 85]]"
      ],
      "metadata": {
        "id": "IEHKq44uZdX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4&nbsp;Create the test DataFrame"
      ],
      "metadata": {
        "id": "CESUTLtArAWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is time for the test set. We will follow the same process."
      ],
      "metadata": {
        "id": "zdeRKvIyZUZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKbYaEuMPqJ3"
      },
      "outputs": [],
      "source": [
        "# Create a copy of the users-items DataFrame and set all values to zero.\n",
        "test = users_items.copy()\n",
        "\n",
        "# Iterate over the nonzero positions in the test dataset.\n",
        "# Get the corresponding rating for each nonzero position from the users_items DataFrame.\n",
        "# Insert that value in the newly created DataFrame at the same position.\n",
        "for column in test.columns:\n",
        "  test[column].values[:] = 0\n",
        "\n",
        "for pos in test_pos.values: \n",
        "    index = pos[0]\n",
        "    col = pos[1]\n",
        "    test.iloc[index, col] = users_items.iloc[index, col]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s784V4rcPqJ4"
      },
      "source": [
        "How many ratings from our user `U1001` fell into the test set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX95-2fAPqJ4"
      },
      "outputs": [],
      "source": [
        "test.iloc[0:1, [31, 32, 75, 81, 85]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eGwGkgBPqJ4"
      },
      "source": [
        "We can build a compact DataFrame to store the positions of all the places in the test set and their true rating."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_test_ratings = []\n",
        "\n",
        "# Iterate over rows and get the values in the two columns (= positions in users_items).\n",
        "# Use positions to get ratings and store them in the true_test_ratings list.\n",
        "for index, row in test_pos.iterrows():\n",
        "  true_test_ratings.append(users_items.iloc[row[0], row[1]])"
      ],
      "metadata": {
        "id": "iZ-LFxDLNLff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSdgf_xuPqJ6"
      },
      "outputs": [],
      "source": [
        "# Add ratings as new column.\n",
        "test_pos = test_pos.assign(true_rating = true_test_ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0KnBUw_PqJ6"
      },
      "outputs": [],
      "source": [
        "test_pos.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgYCnTD8PqJ6"
      },
      "source": [
        "## 2.5&nbsp;Create the similarity matrix for the train set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "fy-wjprnoa_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz0zSQtKPqJ6"
      },
      "outputs": [],
      "source": [
        "# Get cosine similarities for the train dataset.\n",
        "train_similarity = pd.DataFrame(cosine_similarity(train), \n",
        "                                columns=train.index, \n",
        "                                index=train.index)\n",
        "train_similarity.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VC5LNFxPqJ6"
      },
      "source": [
        "## 2.6&nbsp;Predict rating for an individual value in the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will look at the rating that user `U1001` gave to restaurant `placeID=135039` - the data value in position [0, 85] that went into the test dataset.\n",
        "\n",
        "Using only the ratings from the train set and the similarity matrix computed from it, we will predict this value. "
      ],
      "metadata": {
        "id": "p9QePsjsQw56"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLQNTIRPPqJ7"
      },
      "outputs": [],
      "source": [
        "# Get the ratings for restaurant 135039 and the similarities of user U1001.\n",
        "# Combine them in a DataFrame.\n",
        "results = (\n",
        "    pd.DataFrame({\n",
        "        'ratings': train.loc[:,135039], \n",
        "        'similarities' : train_similarity.loc[\"U1001\",:]\n",
        "    })\n",
        ")\n",
        "results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT14Y3rBPqJ7"
      },
      "source": [
        "As always, we compute the weights using the similarities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6BX3hbKPqJ7"
      },
      "outputs": [],
      "source": [
        "# Calculate similarities and add them in a new column \"weights\".\n",
        "results = results.assign(weights = results[\"similarities\"] / (sum(results[\"similarities\"])-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLir3TLYPqJ7"
      },
      "outputs": [],
      "source": [
        "results.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd-1NPAWPqJ7"
      },
      "source": [
        "Then we weigh the rating that each user gave to that restaurant using each user's weight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJqNj2cRPqJ8"
      },
      "outputs": [],
      "source": [
        "results = results.assign(weighted_ratings = results[\"ratings\"] * results[\"weights\"])\n",
        "results.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPq3h2MgPqJ8"
      },
      "source": [
        "Finally, we get the predicted rating for user U1001 for the restaurant `135039` by adding up all the weighted ratings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI5Ey183PqJ8"
      },
      "outputs": [],
      "source": [
        "pred_rating = results[\"weighted_ratings\"].sum()\n",
        "pred_rating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEaEbQ2OPqJ8"
      },
      "source": [
        "Let's have a look at the real rating that user U1001 gace to restaurant `135039`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMUXjlDLPqJ8"
      },
      "outputs": [],
      "source": [
        "true_rating = users_items.loc[\"U1001\", 135039]\n",
        "true_rating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Iy-MuvPqJ8"
      },
      "source": [
        "The difference between the prediction and the true value is the error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASbgJSN5PqJ8"
      },
      "outputs": [],
      "source": [
        "error = true_rating - pred_rating\n",
        "error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtvixyf_PqJ9"
      },
      "source": [
        "# 3.&nbsp;Compute all recommendations for the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0QmKmjxPqJ9"
      },
      "source": [
        "Now we need to predict the rating for all the restaurants in the test set, and compute the performance metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1&nbsp;Create a function to get predictions for individual values"
      ],
      "metadata": {
        "id": "5H8J-Ep6b1YC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2tXowdJPqJ9"
      },
      "source": [
        "We will build a function that computes the ratings for a single user and a single restaurant, taking an index and a column position as input. To do so, we will use the code from above when we predicted the rating of user U1001 for restaurant `135039`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpC3-ZR5PqJ9"
      },
      "outputs": [],
      "source": [
        "def recommender(index_pos, column_pos): \n",
        "    # Build a DataFrame with the ratings for one restaurant (column_pos) and\n",
        "    # the similarities to one user (index_pos).\n",
        "    results = (\n",
        "      pd.DataFrame({\n",
        "          'ratings': train.iloc[:,column_pos], \n",
        "          'similarities' : train_similarity.iloc[index_pos,:]\n",
        "          })\n",
        "      )\n",
        "    \n",
        "    # Compute the weights.\n",
        "    results = results.assign(weights = results[\"similarities\"] / (sum(results[\"similarities\"]) -1))\n",
        "    \n",
        "    # Compute the weighted ratings.\n",
        "    results = results.assign(weighted_ratings = results[\"ratings\"] * results[\"weights\"])\n",
        "    \n",
        "    # Compute the rating prediction for one user and one restaurant.\n",
        "    prediction = results[\"weighted_ratings\"].sum()\n",
        "\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HqA5ST4PqJ9"
      },
      "outputs": [],
      "source": [
        "# Run function for user U1001 and restaurant 135039.\n",
        "recommender(0, 85)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2&nbsp;Apply function to all values in the test set"
      ],
      "metadata": {
        "id": "eim-2NLjdSAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before computing the predicted rating for all rows in the test dataset, let's order the values."
      ],
      "metadata": {
        "id": "xQ7c5WGLdcpU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "uLXpHn97PqJ-"
      },
      "outputs": [],
      "source": [
        "# Sort the values in the test dataset.\n",
        "test_pos.sort_values([\"row_pos\", \"column_pos\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkCwjBB2PqJ-"
      },
      "source": [
        "To get a prediction for all the values in the test dataset, we will iterate over its rows and then store the predicted ratings in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaoGswbhPqJ_"
      },
      "outputs": [],
      "source": [
        "recs_test = []\n",
        "\n",
        "# Iterate over rows of the test_pos dataset.\n",
        "for index, row in test_pos.iterrows():\n",
        "    recs_test.append(\n",
        "# Use recommender function.\n",
        "        recommender(\n",
        "            index_pos = int(row[0]), \n",
        "            column_pos = int(row[1])\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3G3nxYfPqJ_"
      },
      "outputs": [],
      "source": [
        "recs_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBsnEAKVPqJ_"
      },
      "source": [
        "Again, we add the list with the predictions as a new column to the `test_pos` DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV2X5SnMPqKA"
      },
      "outputs": [],
      "source": [
        "# Add new column \"pred_rating\" with the predictions.\n",
        "test_pos = test_pos.assign(pred_rating = recs_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPT5m9NTPqKA"
      },
      "outputs": [],
      "source": [
        "test_pos.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3&nbsp;Use visualizations to compare true and predicted ratings"
      ],
      "metadata": {
        "id": "iClqrx3YlWfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the distributions of both the true and the predicted ratings."
      ],
      "metadata": {
        "id": "iG-KgtaVj123"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG2JimJ9PqKA"
      },
      "outputs": [],
      "source": [
        "# Predicted ratings' distribution first.\n",
        "test_pos.pred_rating.hist();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-JR-58sPqKA"
      },
      "outputs": [],
      "source": [
        "# True ratings' distribution.\n",
        "test_pos.true_rating.hist();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like our predictions are generally much lower than the true ratings. Our small visualizations don't seem to be enough to evaluate the quality of our predictions.\n",
        "\n",
        "Let's try to quantify this."
      ],
      "metadata": {
        "id": "M7Iql0TKlM2C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDGpNhYcPqKB"
      },
      "source": [
        "# 4.&nbsp;Performance metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the various metrics available, let's pick the $R^2$ score first to quantify the difference between the predicted and the true ratings. As a second step, we will use the mean absolute error."
      ],
      "metadata": {
        "id": "6Wxi5fTDlTd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1&nbsp;$R^2$ score"
      ],
      "metadata": {
        "id": "j4xqDzW0VypK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest possible $R^2$ score is 1 while usually, the lower boundary is 0. Nevertheless, the $R^2$ can also become negative when the predicted values perform worse than just using the average score for each prediction would have been.\n",
        "\n",
        "Have a look at the documentation of the $R^2$ score [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html?highlight=r2%20score#sklearn.metrics.r2_score)."
      ],
      "metadata": {
        "id": "zPpSMMThUru7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "IjD1tRYxs3mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGQjuvttPqKD"
      },
      "outputs": [],
      "source": [
        "# Calculate R squared for all predictions and true ratings.\n",
        "r2_score(test_pos.true_rating, test_pos.pred_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A negative $R^2$ score! Let's try a different metric."
      ],
      "metadata": {
        "id": "YpgQQXjenKA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2&nbsp;Mean Absolute Error"
      ],
      "metadata": {
        "id": "NQ3gYImzUvzg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6i37VJNPqKB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vERvehsxPqKB"
      },
      "outputs": [],
      "source": [
        "# Calculate MAE for all predictions and true ratings.\n",
        "mean_absolute_error(test_pos.true_rating, test_pos.pred_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the mean absolute errors for each rating."
      ],
      "metadata": {
        "id": "c4Ff_KLOnH7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "9uXODvGInGWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06p945zFPqKC"
      },
      "outputs": [],
      "source": [
        "plt.title('Error analysis')\n",
        "plt.xlabel('Predicted ratings')\n",
        "plt.ylabel('True ratings')\n",
        "\n",
        "# Plot diagonal for predictions = true ratings.\n",
        "sns.lineplot(x=[0,2], y=[0,2], color='red')\n",
        "# For each test datapoint, plot predicted vs. true rating.\n",
        "sns.scatterplot(x=test_pos[\"pred_rating\"], y=test_pos[\"true_rating\"], alpha=0.4);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that our model won't be capable of exact predictions (i.e. values on the red diagonal). Luckily, this doesn't matter much for recommenders.\n",
        "\n",
        "Instead, we need to be able to rank items from most likely to be enjoyed to least likely.\n",
        "\n",
        "Therefore, it is more important for us that the order is correct. To check for this, we will investigate whether the predicted values for true values of 2 are higher than the predicted values for true values of 1."
      ],
      "metadata": {
        "id": "nPxKDF2wpF7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average predicted score for true ratings of 2."
      ],
      "metadata": {
        "id": "q48dCfZQo43t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raxZC3V3PqKC"
      },
      "outputs": [],
      "source": [
        "# Filter for true ratings of 2.\n",
        "test_pos.loc[test_pos.true_rating==2,:][\"pred_rating\"].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average predicted score for true ratings of 1."
      ],
      "metadata": {
        "id": "-RQsmcfXo96H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7RqdSL6PqKC"
      },
      "outputs": [],
      "source": [
        "# Filter for true ratings of 1.\n",
        "test_pos.loc[test_pos.true_rating==1,:][\"pred_rating\"].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that on average, our recommender predicts higher ratings for restaurants whose true ratings are also higher. This means that in general, our recommender performs reasonably well at finding the correct order."
      ],
      "metadata": {
        "id": "mpfqDZ_ypsPH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwRaf-_APqKD"
      },
      "source": [
        "# 5.&nbsp;Challenge\n",
        "\n",
        "Evaluate whether a recommender system using the sum of `rating + food_rating + service_rating` instead of only the `rating` is better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtluGSrMPqKD"
      },
      "outputs": [],
      "source": [
        "frame.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "6O3aCvJDzbnt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}